
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>OpenEarthMap Few-Shote Challenge</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="./assets/css/app.css">

    <link rel="stylesheet" href="./assets/css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="./assets/js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center"><img src="./assets/img/OpenEarthMap_Logo_v2.png" alt="OpenEarthMap" height="100"></h2>
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <!-- <h4>
                        <b>Submission Servers are open!</b>
                    </h4> -->
                    <image src="./assets/img/img2.webp" class="img-responsive" alt="overview"></image>
                </div>
        </div>
            <!-- <h2><a class="col-md-12 text-center" href=""><img src="./assets/img/OpenEarthMap_Logo_v2.png" alt="OpenEarthMap" height="90"></a></h2> -->
            <h2 class="col-md-12 text-center">
                OpenEarthMap Land Cover Mapping Few-Shot Challenge</br> 
                <small>
                    Co-organized with the <a href="https://sites.google.com/view/l3divu2024/overview" target="_blank">L3D-IVU CVPR 2024</a> Workshop
                </small>
            </h2>
        </div>
        <br>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="#">
                          Submission Portal
                        </a>
                    </li>
                    <li>
                        <a href="#">
                          Dataset Download
                        </a>
                    </li>
                    <li>
                        <a href="#">
                          Baseline Code
                        </a>
                    </li>
                </ul>   
            </div>
        </div>
        <br>

        <div class="row">
                <!-- <div class="col-md-8 col-md-offset-2"> -->
                    <h4 class="col-md-8 col-md-offset-2">
                        <b>Submission portal for development stage (1st Round) is opened!</b>
                    </h4>
                    <!-- <image src="./assets/img/img3.jpg" class="img-responsive" alt="overview"></image>
                </div> -->
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    About
                </h3>
                <p class="text-justify">
                    <!-- The challenge is part of Workshop on Learning with Limited Labelled Data for Image and Video Understanding, held at CVPR 2022. -->
                    The <a href="https://open-earth-map.org/" target="_blank">OpenEarthMap</a> is a remote sensing (RS) semantic segmentation benchmark dataset consisting of 
                    aerial/satellite images covering 97 regions from 44 countries across 6 continents at a spatial resolution of 0.25--0.5m ground sampling
                    distance for global high-resolution land cover mapping. It presents an advancement in geographical diversity and annotation
                    quality over existing benchmarks. OpenEarthMap benchmark models generalize worldwide and they can be used as an off-the-shelf
                    model in a wide range of RS applications.
                </p>
                <p class="text-justify">
                    This challenge is an extension of the 8-class land-cover semantic segmentation task of the OpenEarthMap benchmark.
                    The aim of the challenge is to evaluate and benchmark learning methods for few-shot semantic segmentation on the OpenEarthMap dataset. 
                    The motivation is to enable researchers to develop few-shot learning algorithms for high-resolution RS image semantic segmentation, 
                    which is a very important problem in the applications of RS image analysis.
                </p>
                <!-- <p>The main goal of this challenge is to evaluate and benchmark new and better methods for GQA-LT and VG8K-LT benchmarks 
                    proposed in Exploring Long Tail Visual Relationship Recognition with Large Vocabulary. One high level motivation is
                     to allow researchers to develop methods for long-tail visual relationship recognition - a very important problem for
                     fine-grained image understanding. Both the benchmarks, GQA-LT and VG8K-LT, are long-tailed created out of GQA and VG datasets.
                     With the presence of bounding boxes during both training and test time, the participants are required to submit a csv 
                     file containing the detection results of subject/object and the relation classes in the corresponding triplet.</p> -->
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Important Dates
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            <b>Feb 05, 2024:</b> Opening the challenge submission portal for development phase (First Round).
                        </li>
                        <li>
                            <b>Feb 05, 2024:</b> Opening paper submission (Call for the challenge paper).
                        </li>
                        <li>
                            <b>Mar 22, 2024:</b> Paper submission deadline.
                        </li>
                        <li>
                            <b>Mar 23, 2024:</b> Opening the challenge submission portal for evaluation phase (Second Round).
                        </li>
                        <li>
                            <b>Mar 29, 2024:</b> The challenge submission deadline.
                        </li>
                        <li>
                            <b>Apr 07, 2024:</b> Paper notification to authors.
                        </li>
                        <li>
                            <b>Apr 14, 2024:</b> Camera-ready deadline.
                        </li>
                        <li>
                            <b>Apr 21, 2024:</b> Challenge winner announcement.
                        </li>
                    </ul>
                    Workshop: 19th June, 2024
                </p>
                <hr>
            </div>
        </div>

<!-- Workshop Paper submission deadline: 6th March 2024, 11:59 pm Pacific Time.
Notification to authors: 4th April 2024.
Camera-ready deadline: 14th April 2024. -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Task <!--& Metrics-->
                </h3>
                <!-- <p class="text-justify">
                    The main task for both of these benchmarks is to predict relationship and at the same time subject/object associated with the triplet. During training and evaluation,
                    the ground-truth bounding boxes are already provided and hence the main aim is the classification of sbj/obj and the rel associated with them.
                    The main metric used for the challenge is <i>average per-class accuracy</i>, which is the accuracy of each class calculated separately, then average.
                    The average per-class accuracy is a commonly used metric in the long-tail literature. Further details about the metrics on the leaderboard is give below:
                    
                    <ul>
                        <li>
                            <b>rel_all_per_class:</b> The per-class relationship classification performance averaged across all classes.
                        </li>
                        <li>
                            <b>sbj_obj_all_per_class:</b> The per-class subject/object classification performance averaged across all classes.
                        </li>
                        <li>
                            <b>trip_sro_scores_all:</b> The triplet classification performance averaged across all possible triplet types
                        </li>
                        <li>
                            <b>trip_or_scores_all:</b> The triplet classification performance averaged across all possible object-relation combination types.
                        </li>
                        <li>
                            <b>trip_sr_scores_all:</b> The triplet classification performance averaged across all possible subject-relation combination types.
                        </li>
                        <li>
                            <b>trip_so_scores_all:</b>  The triplet classification performance averaged across all possible subject-object combination types.
                        </li>
                    </ul>
                </p> -->
                <hr>
            </div>
        </div>
                    <!-- We report the results on the subject, object, and relation separately of an (S,R,O) triplet on GQA-LT and VG8K-LT datasets. We evaluate
                    the models using the average per-class accuracy across several frequency bands chosen based on frequency percentiles for GQA-LT and VG8K-LT, which can be seen below:
                </p>
                <center>
                    <image src="img/gqa_split.png" class="img-responsive" alt="overview"></image>
                    <figcaption>Table 1 - GQA-LT splits</figcaption>
                </center>
                <br>
                <center>
                    <image src="img/vg8k_split.png" class="img-responsive" alt="overview"></image>
                    <figcaption>Table 2 - VG8K-LT splits</figcaption>
                </center>
                <br>
                
                <p class="text-justify">
                    Hence the metrics <i>rel_many, rel_med, rel_few, rel_all_per_class</i> are calculated by taking into account Table 1 & 2 for relations, and <i>sbj_obj_many, sbj_obj_med, sbj_obj_few, sbj_obj_all_per_class</i>
                    are calculated by taking into account Table 1 & 2 for sbj/obj. The other two metrics <i>rel_all_per_example</i> and <i>sbj_obj_all_per_example</i> are the standard metrics 
                    that just average individual prediction scores that has been used in the literature. You can also find more info about the dataset and the metrics in the <a href='docs/supplementary_document.pdf'>supplementary document</a>.
                </p>
                <hr>
            </div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Rules <!--Rules & Subission Format-->
                </h3>
                <!-- <p class="text-justify">
                    The submission is supposed to be a csv file (named <i>rel_detections_gt_boxes_prdcls.csv</i> when the given evaluation code is run on starter code) that is to be renamed 
                    <b>answer.csv</b> and compressed into a .zip file (named <b>submission.zip</b>) before submission. No other format of submission will be accepted.
                </p> -->
                <hr>
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Subission Format
                </h3>
                <p class="text-justify">
                    The submission is supposed to be a csv file (named <i>rel_detections_gt_boxes_prdcls.csv</i> when the given evaluation code is run on starter code) that is to be renamed 
                    <b>answer.csv</b> and compressed into a .zip file (named <b>submission.zip</b>) before submission. No other format of submission will be accepted.
                </p>
                <hr>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Metrics <!--Submission Portal-->
                </h3>
                <!-- <p class="text-justify">
                    <ul>
                        <li>
                            <b>Challenge 1:</b> GQA-LT Benchmark: <a href="https://codalab.lisn.upsaclay.fr/competitions/10821">Link</a>
                        </li>
                        <li>
                            <b>Challenge 2:</b> VG8K-LT Benchmark: <a href="https://codalab.lisn.upsaclay.fr/competitions/10822">Link</a>
                        </li>
                    </ul>
                </p> -->
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Dataset
                </h3>
                <!-- <p class="text-justify">
                    <ul>
                        <li>
                            <b>GQA-LT:</b> <br> You can download the annotations and splits necessary for the GQA-LT benchmark <a href="https://drive.google.com/file/d/1ypmMOq2TkZyLNVuU9agHS7_QcsfTtBmn/view?usp=sharing">here</a>.
                            You should see a 'gvqa' folder once unzipped. It contains seed folder called 'seed0' that contains .json annotations suited for the dataloader used in our implementations. Also, download GQA images from <a href="https://cs.stanford.edu/people/dorarad/gqa/download.html">here</a>
                        </li>
                        <li>
                            <b>VG8K-LT:</b> <br> You can download the annotations and splits necessary for the VG8K-LT benchmark <a href="https://drive.google.com/file/d/1S8WNnK0zt8SDAGntkCiRDfJ8rZOR3Pgx/view?usp=sharing">here</a>.
                            You should see a 'vg8k' folder once unzipped. It contains seed folder called 'seed3' that contains .json annotations suited for the dataloader used in our implementations. Also, download VG images from <a href="https://visualgenome.org/api/v0/api_home.html">here</a>
                        </li>
                    </ul>

                    Further instructions about the dataset placing and also dataloader can be found in tthe README of starter code <a href="https://github.com/Vision-CAIR/LTVRR/tree/challenge-submission">here</a>.

                </p> -->
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Baseline Model <!--Submission Portal-->
                </h3>
                <!-- <p class="text-justify">
                    <ul>
                        <li>
                            <b>Challenge 1:</b> GQA-LT Benchmark: <a href="https://codalab.lisn.upsaclay.fr/competitions/10821">Link</a>
                        </li>
                        <li>
                            <b>Challenge 2:</b> VG8K-LT Benchmark: <a href="https://codalab.lisn.upsaclay.fr/competitions/10822">Link</a>
                        </li>
                    </ul>
                </p> -->
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Subission Format <!--Baseline Code-->
                </h3>
                <!-- <p class="text-justify">
                    You can find the starter code that can be used for submissions <a href="https://github.com/Vision-CAIR/LTVRR/tree/ltvrd-challenge-2023">here</a>. <i>Please follow the code as presented in the branch 'ltvrd-challenge-2023' for 
                    creating an output in the format that is required for the submission</i>. For running the baseline models you can follows the README 
                    instructions. You can also download some of the pre-trained models present there. After running the test file (here <i>test_net_rel.py</i>), the csv file to be used for submission would be named <i>rel_detections_gt_boxes_prdcls.csv</i>, 
                    which to be renamed according to rules mentioned above. For any query regarding this, please feel free to contact the people mentioned below.
                </p> -->
                <hr>
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Contact
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            <a href="mailto:goel.arushi@gmail.com">goel.arushi@gmail.com</a>
                        </li>
                        <li>
                            <a href="mailto:aagarwal@ma.iitr.ac.in">aagarwal@ma.iitr.ac.in</a>
                        </li>
                        <li>
                            <a href="mailto:jun.chen@kaust.edu.sa">jun.chen@kaust.edu.sa</a>
                        </li>
                        <li>
                            <a href="mailto:mohamed.elhoseiny@kaust.edu.sa">mohamed.elhoseiny@kaust.edu.sa</a>
                        </li>
                    </ul>
                </p>
                <hr>
            </div>
        </div> -->
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Organizers
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            Junshi Xia, RIKEN-AIP, Japan (<a href="mailto:junshi.xia@riken.jp">junshi.xia@riken.jp</a>)
                        </li>
                        <li>
                            Clifford Broni-Bediako, RIKEN-AIP, Japan (<a href="mailto:clifford.broni-bediako@riken.jp">clifford.broni-bediako@riken.jp</a>)
                        </li>
                        <li>
                            Jian Song, The Unversity of Tokyo, Japan 
                        </li>
                        <li>
                            Hongruixuan Chen, The Unversity of Tokyo, Japan (<a href="mailto:qschrx@g.ecc.u-tokyo.ac.jp">qschrx@g.ecc.u-tokyo.ac.jp</a>)
                        </li>
                        <li>
                            Naoto Yokaya, The Unversity of Tokyo/RIKEN-AIP, Japan (<a href="mailto:yokoya@k.u-tokyo.ac.jp">yokoya@k.u-tokyo.ac.jp</a>)
                        </li>
                    </ul>
                </p>
                <hr>
            </div>
        </div>
        
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <!-- <textarea id="bibtex" class="form-control" readonly>
                    @inproceedings{ltvrr2021,
                        title={Exploring Long Tail Visual Relationship Recognition with Large Vocabulary},
                        author={Abdelkarim, Sherif and Agarwal, Aniket and Achlioptas, Panos and Chen, Jun and Huang, Jiaji and Li, Boyang and Church, Kenneth and Elhoseiny, Mohamed},
                        booktitle={Proceedings of the IEEE International Conference on Computer Vision},
                        year={2021}
                        }
                    </textarea> -->
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p>
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>

    </div>
</body>
</html>
