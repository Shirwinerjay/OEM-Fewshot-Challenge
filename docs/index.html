
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>OpenEarthMap Few-Shote Challenge</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="./assets/css/app.css">

    <link rel="stylesheet" href="./assets/css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="./assets/js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center"><img src="./assets/img/OpenEarthMap_Logo_v2.png" alt="OpenEarthMap" height="100"></h2>
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <!-- <h4>
                        <b>Submission Servers are open!</b>
                    </h4> -->
                    <image src="./assets/img/img2.webp" class="img-responsive" alt="overview"></image>
                </div>
        </div>
            <!-- <h2><a class="col-md-12 text-center" href=""><img src="./assets/img/OpenEarthMap_Logo_v2.png" alt="OpenEarthMap" height="90"></a></h2> -->
            <h2 class="col-md-12 text-center">
                OpenEarthMap Land Cover Mapping Few-Shot Challenge</br> 
                <small>
                    Co-organized with the <a href="https://sites.google.com/view/l3divu2024/overview" target="_blank">L3D-IVU CVPR 2024</a> Workshop
                </small>
            </h2>
        </div>
        <br>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="#">
                          Codalab Submission Portal
                        </a>
                    </li>
                    <li>
                        <a href="#">
                          Dataset Download
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/cliffbb/OEM-Fewshot-Challenge">
                          Baseline Code
                        </a>
                    </li>
                    <li>
                        <a href="#paper">
                            Paper Submission
                        </a>
                    </li>
                </ul>   
            </div>
        </div>
        <br>

        <div class="row">
                <!-- <div class="col-md-8 col-md-offset-2"> -->
                    <h4 class="col-md-8 col-md-offset-2">
                        <!-- <b>Submission portal for development stage is opened!</b> -->
                    </h4>
                    <!-- <image src="./assets/img/img3.jpg" class="img-responsive" alt="overview"></image>
                </div> -->
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    About the Challenge
                </h3>
                <p class="text-justify">
                    <!-- The challenge is part of Workshop on Learning with Limited Labelled Data for Image and Video Understanding, held at CVPR 2022. -->
                    The <a href="https://open-earth-map.org/" target="_blank">OpenEarthMap</a> is a remote sensing (RS) semantic segmentation 
                    benchmark dataset consisting of aerial/satellite images covering 97 regions from 44 countries across 6 continents at a spatial resolution
                    of 0.25--0.5m ground sampling distance for global high-resolution land cover mapping. It presents an advancement in geographical 
                    diversity and annotation quality, enabling models to generalize worldwide. This challenge extends the land-cover semantic 
                    segmentation task of the OpenEarthMap benchmark to few-shot multi-class semantic segmentation.
                </p>
                <p class="text-justify">
                    The aim of the challenge is to evaluate and benchmark learning methods for few-shot semantic segmentation on the OpenEarthMap dataset,
                    to promote research on geoinformatics for social good. The motivation is to enable researchers to develop few-shot learning algorithms 
                    for high-resolution RS image semantic segmentation, which is a very important problem in the various applications of RS image analysis 
                    such as disaster response, urban planning, and natural resource management.
                </p>
                <p class="text-justify">
                    Scientific papers describing the best entries will be included and presented at the 
                    <a href="https://sites.google.com/view/l3divu2024/overview" target="_blank">Workshop on Learning with Limited Labelled Data 
                    for Image and Video Understanding (L3D-IVU)</a>, and published in the CVPR 2024 Workshop Proceedings. 
                </p> 
                    <!-- The 2021 IEEE GRSS Data Fusion Contest, organized by the Image Analysis and Data Fusion Technical Committee, aims to promote research
                     on geospatial AI for social good. The global objective is to build models for understanding the state and changes of artificial
                      and natural environments from multimodal and multitemporal remote sensing data towards sustainable developments. The 2021 Data Fusion 
                      Contest will consist of two challenge tracks: Track 1: Automated landcover change detection and classification Track 2: End the darkness:
                       Detection of human settlements deprived of access to electricity Scientific papers describing the best entries will be included in
                       the Technical Program of IGARSS 2021, presented in an invited session “IEEE GRSS Data Fusion Contest,” and published in the IGARSS 2021 Proceedings.

                     In this challenge, we introduce \textit{new} land-cover classes 
                    %({\textit{river, vehicle, sports venue, parking space, beach, railway, and fallow agriculture field}}) 
                    into the OpenEarthMap dataset for multi-class few-shot RS semantic segmentation. Given the \textit{new} land-cover classes, the base classes 
                    ${C_{train}}$ from which the meta-training set $\mathcal{D}_{train}$ is created and the target classes  $C_{test}$ from which the
                     meta-test set $\mathcal{D}_{test}$ is created are disjoint sets (i.e., $C_{train}\ \cap\ C_{test} = \emptyset$). In a few-shot learning 
                     setting of \textit{N}-way-\textit{K}-shot, the participants are required to submit the multi-class semantic segmentation results 
                     of \textit{Q} query images (unlabelled) from the target classes $C_{test}$ using a support set with \textit{K} labelled images.
                      The support \textit{K} labelled images (a.k.a train-set in the meta-test time episodes) are also from the target classes  

                      For evaluation, we resort to the mean intersection-over-union (mIoU), which is the average value over the IoUs of all the target classes.
                       Note that the mIoU is computed over the target classes only without the background. The setup code will provide a framework for generating
                        a file in the format that is compatible with the evaluation server including all the metrics (class IoUs and mIoU) that are required.
                         The challenge will be hosted on the Codalab platform. A template of the challenge page along with the server link can be
                          found \href{https://codalab.lisn.upsaclay.fr/competitions/9121}{here}. The top-performing team will be awarded with certificates and prizes. 
                          The prize award is still to be decided. The submitted papers will be peer-reviewed and the accepted ones, including non-winning teams' 
                          papers, will be published in conjunction with CVPR 2024 proceedings. 

                     The track aims to promote innovation in detection of human settlements deprived of access to electricity, as well as to provide objective
                    and fair comparisons among methods. The ranking is based on quantitative accuracy parameters computed with respect to undisclosed test samples. 
                    Participants will be given a limited time to submit their land cover maps after the competition started.   -->
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Competition Phases
                </h3>
                <p class="text-justify">
                    The contest consists of two phases:</br>
                    <b>Phase 1 (Development Phase):</b> Participants are provided with the OpenEarthMap Few-Shot dataset for training and validation to train and 
                    validate their algorithms (<i>See the <a href="#dataset">dataset section</a> for detailed description of the training set and validation set</i>).
                    The participants can submit results on the validation set to the <a href="#" target="_blank">Codalab competition submission portal</a> 
                    to get feedback on the performance. The performance of the last submission from each account will be displayed on the leaderboard. 
                    In parallel, participants submit a <a href="#paper">short paper (4-page manuscript)</a> of the approach used to be eligible to enter Phases 2.
                    The manuscripts will be included in the CVPR 2024 Workshop Proceedings.</br>
                    <b>Phase 2 (Evaluation Phase):</b> Participants receive the testing set of the OpenEarthMap Few-Shot dataset 
                    (<i>See the <a href="#dataset">dataset section</a> for detailed description of the testing set</i>). The participants submit their results on 
                    the testing set to the <a href="#" target="_blank">Codalab competition submission portal</a> within <b>six days</b> from the release of the testing set.
                    After evaluation of the results, the top three winners are announced.
                </p>
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Important Dates
                </h3>
                <p class="text-justify">
                    All the deadlines are strict, no extension will be given (11:59pm Pacific Time).
                    <ul>
                        <li>
                            <b>Feb 05, 2024:</b> Opening the challenge submission portal for the development phase.
                        </li>
                        <li>
                            <b>Feb 05, 2024:</b> Opening paper submission (Call for the challenge paper).
                        </li>
                        <li>
                            <b>Mar 22, 2024:</b> Paper submission deadline (Submission deadline for the challenge paper).
                        </li>
                        <li>
                            <b>Mar 23, 2024:</b> Opening the challenge submission portal for the evaluation phase.
                        </li>
                        <li>
                            <b>Mar 29, 2024:</b> Challenge submission deadline.
                        </li>
                        <li>
                            <b>Apr 07, 2024:</b> Paper notification to authors.
                        </li>
                        <li>
                            <b>Apr 14, 2024:</b> Camera-ready deadline.
                        </li>
                        <li>
                            <b>Apr 21, 2024:</b> Challenge winner announcement.
                        </li>
                    </ul>
                    L3D-IVU Workshop: Tuesday June 18, 2024.
                </p>
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Task & Metrics
                </h3>
                <!-- <p class="text-justify">
                    The main task for both of these benchmarks is to predict relationship and at the same time subject/object associated with the triplet. During training and evaluation,
                    the ground-truth bounding boxes are already provided and hence the main aim is the classification of sbj/obj and the rel associated with them.
                    The main metric used for the challenge is <i>average per-class accuracy</i>, which is the accuracy of each class calculated separately, then average.
                    The average per-class accuracy is a commonly used metric in the long-tail literature. Further details about the metrics on the leaderboard is give below:
                    
                    <ul>
                        <li>
                            <b>rel_all_per_class:</b> The per-class relationship classification performance averaged across all classes.
                        </li>
                        <li>
                            <b>sbj_obj_all_per_class:</b> The per-class subject/object classification performance averaged across all classes.
                        </li>
                        <li>
                            <b>trip_sro_scores_all:</b> The triplet classification performance averaged across all possible triplet types
                        </li>
                        <li>
                            <b>trip_or_scores_all:</b> The triplet classification performance averaged across all possible object-relation combination types.
                        </li>
                        <li>
                            <b>trip_sr_scores_all:</b> The triplet classification performance averaged across all possible subject-relation combination types.
                        </li>
                        <li>
                            <b>trip_so_scores_all:</b>  The triplet classification performance averaged across all possible subject-object combination types.
                        </li>
                    </ul>
                </p> -->
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Rules of the Game <!--Rules & Subission Format-->
                </h3>
                <!-- <p class="text-justify">
                    The submission is supposed to be a csv file (named <i>rel_detections_gt_boxes_prdcls.csv</i> when the given evaluation code is run on starter code) that is to be renamed 
                    <b>answer.csv</b> and compressed into a .zip file (named <b>submission.zip</b>) before submission. No other format of submission will be accepted.
                </p> -->
                <!-- <p class="text-justify">
                    You can find the starter code that can be used for submissions <a href="https://github.com/Vision-CAIR/LTVRR/tree/ltvrd-challenge-2023">here</a>. <i>Please follow the code as presented in the branch 'ltvrd-challenge-2023' for 
                    creating an output in the format that is required for the submission</i>. For running the baseline models you can follows the README 
                    instructions. You can also download some of the pre-trained models present there. After running the test file (here <i>test_net_rel.py</i>), the csv file to be used for submission would be named <i>rel_detections_gt_boxes_prdcls.csv</i>, 
                    which to be renamed according to rules mentioned above. For any query regarding this, please feel free to contact the people mentioned below.
                </p> -->
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 id="dataset">
                    The Dataset
                </h3>
                <!-- <p class="text-justify">
                    The train set consists of images and their labels, which contains only the <i>base classes</i> for training the base model.
                    <ul>
                        <li>
                            <b>GQA-LT:</b> <br> You can download the annotations and splits necessary for the GQA-LT benchmark <a href="https://drive.google.com/file/d/1ypmMOq2TkZyLNVuU9agHS7_QcsfTtBmn/view?usp=sharing">here</a>.
                            You should see a 'gvqa' folder once unzipped. It contains seed folder called 'seed0' that contains .json annotations suited for the dataloader used in our implementations. Also, download GQA images from <a href="https://cs.stanford.edu/people/dorarad/gqa/download.html">here</a>
                        </li>
                        <li>
                            <b>VG8K-LT:</b> <br> You can download the annotations and splits necessary for the VG8K-LT benchmark <a href="https://drive.google.com/file/d/1S8WNnK0zt8SDAGntkCiRDfJ8rZOR3Pgx/view?usp=sharing">here</a>.
                            You should see a 'vg8k' folder once unzipped. It contains seed folder called 'seed3' that contains .json annotations suited for the dataloader used in our implementations. Also, download VG images from <a href="https://visualgenome.org/api/v0/api_home.html">here</a>
                        </li>
                    </ul>

                    Further instructions about the dataset placing and also dataloader can be found in tthe README of starter code <a href="https://github.com/Vision-CAIR/LTVRR/tree/challenge-submission">here</a>.

                </p> -->
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Baseline Model <!--Submission Portal-->
                </h3>
                <!-- <p class="text-justify">
                    <ul>
                        <li>
                            <b>Challenge 1:</b> GQA-LT Benchmark: <a href="https://codalab.lisn.upsaclay.fr/competitions/10821">Link</a>
                        </li>
                        <li>
                            <b>Challenge 2:</b> VG8K-LT Benchmark: <a href="https://codalab.lisn.upsaclay.fr/competitions/10822">Link</a>
                        </li>
                    </ul>
                </p> -->
                <hr>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results, Awards, & Prizes
                </h3>
                <!-- <p class="text-justify">
                    
                    The following eight teams will be declared as winners:
                    The first, second, third, and fourth ranked teams in Track 1
                    The first, second, third, and fourth ranked teams in Track 2
                    The authors of the eight winning submissions will:
                    Present their manuscripts in an invited session dedicated to the Contest at IGARSS 2021
                    Publish their manuscripts in the Proceedings of IGARSS 2019
                    Be awarded IEEE Certificates of Recognition.
                    The authors of the first and second ranked teams in both tracks will:
                    Receive as a prize (TBD!!).
                    Co-author journal papers (in a limit of 3 co-authors per submission), which will summarize the outcome of the Contest 
                    and will be submitted to IEEE JSTARS. To maximize impact and promote research in geospatial AI for social good, 
                    the open-access option will be used for this journal submission.
                    
                    Top ranked teams will be awarded during IGARSS 2021, Brussels, Belgium in July 2020. The costs for open-access
                     publication will be supported by the GRSS. The winner team prize is kindly sponsored by the IGARSS 2021 team.
                    
                </p> -->
                <hr>
            </div>
        </div>
            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3 id="paper">
                    Challenge Paper Submission
                </h3>
                <!-- <p class="text-justify">
                    Following this, they will have one month to write their manuscript that will be included in the IGARSS proceedings. Manuscripts are 4-page 
                    IEEE-style formatted. Each manuscript describes the addressed problem, the proposed method, and the experimental results.

                   Important Dates:
                    Workshop Paper submission deadline: 6th March 2024, 11:59 pm Pacific Time.
                    Notification to authors: 4th April 2024.
                    Camera-ready deadline: 14th April 2024.
                    CMT3 Submission: https://cmt3.research.microsoft.com/L3DIVUCVPR2024/ 
                    We encourage submissions that are under one of the topics of interest, but also we welcome other interesting and 
                    relevant research for learning with limited labelled data.
                    Accepted papers will be presented at the poster session, some as orals, and one paper will be awarded as the best paper.
                    Submission Guidelines:
                    We accept submissions of max 8 pages (excluding references). We encourage authors to submit a 4-page work as well.
                    We accept dual submissions to CVPR 2024 and L3D-IVU 2024.
                    Submitted manuscript should follow the CVPR 2024 paper template.
                    Submissions will be rejected without review if they:
                    Contain more than 8 pages (excluding references).
                    Violate the double-blind policy.
                    Violate the dual-submission policy for papers with more than 4 pages excluding references.
                    The accepted papers will be in the main conference workshops proceedings if the authors agree (this option 
                    is valid only for full-length papers not published at CVPR 2024).
                    Short papers will be linked at the workshop webpage but will not be in the main conference proceedings.
                    Papers will be peer-reviewed under double-blind policy and must be submitted online through the CMT submission system.

                    Following this, they will have one month to write their manuscript that will be included in the IGARSS proceedings. Manuscripts are 4-page 
                    IEEE-style formatted. Each manuscript describes the addressed problem, the proposed method, and the experimental results.
                </p> -->
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Organizers
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            Junshi Xia, RIKEN-AIP, Japan (<a href="mailto:junshi.xia@riken.jp">junshi.xia@riken.jp</a>)
                        </li>
                        <li>
                            Clifford Broni-Bediako, RIKEN-AIP, Japan (<a href="mailto:clifford.broni-bediako@riken.jp">clifford.broni-bediako@riken.jp</a>)
                        </li>
                        <li>
                            Jian Song, The Unversity of Tokyo, Japan 
                        </li>
                        <li>
                            Hongruixuan Chen, The Unversity of Tokyo, Japan (<a href="mailto:qschrx@g.ecc.u-tokyo.ac.jp">qschrx@g.ecc.u-tokyo.ac.jp</a>)
                        </li>
                        <li>
                            Naoto Yokaya, The Unversity of Tokyo/RIKEN-AIP, Japan (<a href="mailto:yokoya@k.u-tokyo.ac.jp">yokoya@k.u-tokyo.ac.jp</a>)
                        </li>
                    </ul>
                </p>
                <hr>
            </div>
        </div>
        
        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
                    @inproceedings{ltvrr2021,
                        title={Exploring Long Tail Visual Relationship Recognition with Large Vocabulary},
                        author={Abdelkarim, Sherif and Agarwal, Aniket and Achlioptas, Panos and Chen, Jun and Huang, Jiaji and Li, Boyang and Church, Kenneth and Elhoseiny, Mohamed},
                        booktitle={Proceedings of the IEEE International Conference on Computer Vision},
                        year={2021}
                        }
                    </textarea>
                </div>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p style="font-size: x-small;">
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>

    </div>
</body>
</html>
